# ChatD Internships Bot - TODO & Future Improvements

This document tracks planned improvements and enhancements for the ChatD Internships Discord bot.

## üöÄ Priority Items

### 1. Dynamic Log Level Control ‚úÖ **COMPLETED**
**Goal**: Update logging levels on the fly without process restart

**Current Issue**: ~~Changing log levels requires systemctl restart, causing service interruption~~ **RESOLVED**

**Implementation Plan**:
- [x] **1.1** Enhance signal handlers in `logging_utils.py`
  - [x] Added SIGHUP signal handler for direct level changes
  - [x] Docker-compatible signal sending via `docker kill --signal=HUP`
  - [x] File-based level communication system for clean Docker integration
- [x] **1.2** Add management command support
  - [x] Added `chatd-loglevel` command with full level support
  - [x] Supports all levels: `debug|info|warning|error|critical`
  - [x] Clean, intuitive command interface with helpful error messages
- [x] **1.3** Simplified implementation
  - [x] Removed complex incremental up/down signal handlers
  - [x] Single, clear approach for direct level setting
  - [x] Professional command-line interface matching standard tools
- [x] **1.4** Document runtime log level control
  - [x] Added comprehensive usage documentation
  - [x] Updated management scripts help text

**Results Achieved**:
- **Instant log level changes**: Change from INFO to DEBUG without any service interruption
- **Professional interface**: `sudo chatd-loglevel debug` provides immediate verbose logging
- **Production-ready**: Enables instant troubleshooting of live issues
- **Clean implementation**: Single signal handler approach, no complexity
- **Full level support**: All 5 standard log levels (debug/info/warning/error/critical)

**Files modified**: `chatd/logging_utils.py`, `scripts/create-management-scripts.sh`

### 2. Optimize Docker Build Performance ‚úÖ **COMPLETED**
**Goal**: Separate build and run phases to eliminate slow startup times

**Current Issue**: ~~`systemctl start` rebuilds Docker image every time (~30-60 seconds)~~ **RESOLVED**

**Implementation Plan**:
- [x] **2.1** Modify systemd service strategy
  - [x] Remove `ExecStartPre` Docker build step
  - [x] Create separate build workflow
- [x] **2.2** Add build management commands
  - [x] `chatd build`: Manual rebuild trigger
  - [x] `chatd deploy`: Deploy with existing image
  - [x] `chatd update`: Build + restart in one command
- [x] **2.3** Implement image versioning
  - [x] Tag images with git commit hash: `chatd-internships:${GIT_COMMIT}`
  - [x] Track current deployment version
- [x] **2.4** Enhanced management tooling
  - [x] User-agnostic build process with CHATD_BRANCH environment variable
  - [x] Enhanced management scripts with better error handling
  - [x] Build verification and deployment validation
- [x] **2.5** Update systemd service file
  - [x] Remove build steps from service startup
  - [x] Add health checks for faster failure detection

**Results Achieved**:
- **Deployment time**: Reduced from ~4+ minutes to ~8 seconds
- **Build separation**: Can now build once, deploy multiple times
- **Smart builds**: Skip rebuilds when no code changes (~0.5 seconds)
- **Version tracking**: Git commit-based image tagging and management
- **New commands**: `chatd build`, `chatd deploy`, `chatd update`, `chatd version`
- **Faster iteration**: Quick deployments for testing and rollbacks
- **Enhanced tooling**: User-agnostic build process with CHATD_BRANCH support

**Files modified**: `chatd-internships.service`, `scripts/create-management-scripts.sh`

### 3. Docker Image Auto-Pruning ‚úÖ **COMPLETED**
**Goal**: Automatically clean up old Docker images to prevent disk space issues

**Current Issue**: ~~Multiple Docker images accumulate (386MB each), consuming limited disk space~~ **RESOLVED**
**Current State**: ~~3 images = ~1.1GB, only 505MB free space remaining~~ **MANAGED**

**Implementation Plan**: ‚úÖ **COMPLETED**
- [x] **3.1** Add auto-pruning to deployment workflow
  - [x] Modify `chatd-update` script to retain only N latest versions (default: 3)
  - [x] Keep current commit and 2 previous versions (N-1 and N-2 rollback capability)
  - [x] Automatic cleanup after successful deployment
  - [x] Configuration option for retention count: `DOCKER_IMAGE_RETENTION=3`
- [x] **3.2** Manual cleanup commands
  - [x] Add `chatd cleanup` command for manual image pruning
  - [x] Add `chatd images` command to list current images with sizes
  - [x] Add `chatd prune` command for aggressive cleanup (keep only latest)
  - [x] Dry-run mode: `chatd cleanup --dry-run` to preview what would be deleted
- [x] **3.3** Disk space monitoring integration
  - [x] Check available disk space before building new images
  - [x] Automatic emergency cleanup if disk usage > 90%
  - [x] Warning messages when approaching space limits
  - [x] Integration with future monitoring dashboard alerts

**Example Auto-Pruning Logic**:
```bash
# In chatd-update script, after successful deployment:
echo "üßπ Cleaning up old Docker images..."
RETENTION_COUNT=${DOCKER_IMAGE_RETENTION:-3}

echo "üìä Retention policy: keeping $RETENTION_COUNT images (current + 2 rollback options)"

# Get all chatd images sorted by creation date (newest first)
IMAGES=$(docker images chatd-internships --format "{{.Tag}}" | head -n +$RETENTION_COUNT)

# Remove images older than retention count
docker images chatd-internships --format "{{.Tag}}" | tail -n +$((RETENTION_COUNT + 1)) | while read tag; do
    if [[ "$tag" != "latest" ]]; then
        echo "üóëÔ∏è  Removing old image: chatd-internships:$tag"
        docker rmi "chatd-internships:$tag" 2>/dev/null || true
    fi
done

echo "‚úÖ Cleanup complete. Retained $RETENTION_COUNT images."
```

**New Management Commands**:
```bash
# Image management
sudo chatd images                    # List all ChatD images with sizes
sudo chatd cleanup                   # Clean up old images (keep 3)
sudo chatd cleanup --count 5         # Keep 5 images instead of 3
sudo chatd cleanup --dry-run         # Preview what would be deleted
sudo chatd prune                     # Aggressive cleanup (keep only latest)

# Disk space monitoring
sudo chatd disk                      # Show disk usage and image sizes
sudo chatd disk --alert              # Check if cleanup needed
```

**Disk Space Recovery**:
- **Current**: 3 images √ó 386MB = ~1.1GB used
- **After cleanup**: 3 images √ó 386MB = ~1.1GB (prevents growth beyond 3 images)
- **Emergency cleanup**: 1 image √ó 386MB = ~770MB total recovered

**Time to Implement**: ~30-45 minutes (high impact, low effort)

**Files to modify**:
- `scripts/create-management-scripts.sh` (add new cleanup commands)

**Results Achieved**:
- **Prevent disk space exhaustion** with current 7GB constraint
- **Automatic maintenance** - no manual intervention needed
- **Configurable retention** - balance between rollback capability and space
- **Emergency cleanup** - automatic recovery from space issues
- **Better visibility** - commands to monitor image usage

**Files Modified**: `scripts/create-management-scripts.sh`

### 4. Asynchronous Message Reactions
**Goal**: Improve reaction performance through async processing

**Current Issue**: Adding reactions blocks message sending, slowing overall performance

**Implementation Plan**:
- [ ] **4.1** Refactor reaction logic in `bot.py`
  - [ ] Move reactions to background task queue
  - [ ] Use `asyncio.create_task()` for non-blocking reactions
- [ ] **4.2** Implement reaction batching
  - [ ] Queue reactions and process in batches
  - [ ] Add configurable delay between reaction batches
- [ ] **4.3** Add reaction failure handling
  - [ ] Retry logic for failed reactions
  - [ ] Graceful degradation when reactions fail
- [ ] **4.4** Configuration options
  - [ ] `REACTION_BATCH_SIZE=10`
  - [ ] `REACTION_DELAY_MS=100`
  - [ ] `REACTION_RETRY_COUNT=3`

**Files to modify**: `chatd/bot.py`, `chatd/config.py`

## üéØ Feature Enhancements

### 5. Smart Reaction-Based Info Sharing
**Goal**: Enhanced info messages triggered by specific reactions

**Current Behavior**: All reactions trigger DM with job details
**Target Behavior**: Only '‚ùì' reaction triggers enhanced company info

**Implementation Plan**:
- [ ] **5.1** Update reaction handler logic
  - [ ] Check reaction emoji type before processing
  - [ ] Only process '‚ùì' emoji for detailed info
- [ ] **5.2** Enhanced company information gathering
  - [ ] Query `listings.json` for same company roles
  - [ ] Filter by configurable time window (default: 7 days)
  - [ ] Group by company and format nicely
- [ ] **5.3** Rich DM formatting
  - [ ] Company overview section
  - [ ] All recent roles from company
  - [ ] Application deadlines and dates
  - [ ] Direct links to applications
- [ ] **5.4** Configuration options
  - [ ] `COMPANY_INFO_DAYS=7`
  - [ ] `INFO_REACTION_EMOJI=‚ùì`
  - [ ] `ENABLE_COMPANY_INFO=true`

**Files to modify**: `chatd/bot.py`, `chatd/messages.py`, `chatd/config.py`

### 6. Configurable Date Filtering ‚úÖ **COMPLETED**
**Goal**: Make "too old" threshold configurable instead of hardcoded

**Current Implementation**: ~~Hardcoded 5-day filter in code~~ **RESOLVED**
**Target**: Environment variable configuration ‚úÖ **ACHIEVED**

**Implementation Plan**:
- [x] **6.1** Add configuration variable ‚úÖ **COMPLETED**
  - [x] `MAX_POST_AGE_DAYS=5` in `.env`
  - [x] Update `config.py` to load this setting
- [x] **6.2** Update filtering logic ‚úÖ **COMPLETED**
  - [x] Replace hardcoded values in message processing
  - [x] Apply consistently across all date checks
- [x] **6.3** Add validation ‚úÖ **COMPLETED**
  - [x] Ensure positive integer values
  - [x] Reasonable bounds (1-30 days)
- [x] **6.4** Document in README ‚úÖ **COMPLETED**
  - [x] Explain impact of different values
  - [x] Added to .env.example

**Results Achieved**:
- **Configurable filtering**: MAX_POST_AGE_DAYS environment variable
- **Backward compatible**: Defaults to 5 days (existing behavior)
- **Validation**: 1-30 day range with helpful error messages
- **Flexible operations**: Can adjust based on deployment needs
- **Enhanced logging**: Shows configured max age in debug output

**Files modified**: `chatd/config.py`, `chatd/bot.py`, `.env.example`

## üîç Data Quality & Performance

### 7. Listings Data Audit
**Goal**: Comprehensive audit of matching logic accuracy

**Scope**: Ensure no false positives or missed matches in role detection

**Status**: **COMPLETED** ‚úÖ - ID-based tracking implementation eliminates matching complexity

**Implementation Results**:
- [x] **7.1** Critical architectural improvement
  - [x] Migrated from composite key matching to direct UUID-based tracking
  - [x] Eliminated complex `get_role_id()` and `normalize_role_key()` functions
  - [x] Now uses direct `role['id']` access from listings.json (100% unique coverage)
  - [x] Production data migration completed (319/325 messages migrated successfully)
- [x] **7.2** Enhanced system reliability
  - [x] Removed over-matching issues that prevented re-opening detection
  - [x] Simplified codebase from complex matching logic to straightforward ID lookups
  - [x] Validated 9,747 unique IDs in listings.json with 100% uniqueness
  - [x] Updated unit tests and removed obsolete matching test cases
- [x] **7.3** Production deployment
  - [x] Successful migration script execution with comprehensive backup
  - [x] Enhanced management scripts with user-agnostic build process
  - [x] Docker build verification and deployment to production
  - [x] Bot now operational with new ID-based architecture

**Key Improvement Made**:
```python
# OLD: Complex matching with potential issues
def get_role_id(role_data):
    # 28-line function with normalization and composite keys
    return f"{company}__{title}__{date_posted}"

# NEW: Direct UUID access (bulletproof)
role_id = role['id']  # Direct access to unique UUID from listings.json
```

**Results Achieved**:
- **100% reliable tracking**: Direct UUID access eliminates matching ambiguity
- **Simplified maintenance**: Removed 50+ lines of complex matching code
- **Production ready**: Successfully deployed with migrated data
- **Future-proof**: ID-based system scales with listings.json growth

**Files modified**: `chatd/repo.py`, `chatd/bot.py`, `scripts/migrate-to-id-keys.py`

### 8. Efficient Delta Processing
**Goal**: Process only changes instead of full file comparison

**Current Approach**: Full file read and comparison on every check
**Target**: Git diff-based change detection

**Implementation Plan**:
- [ ] **8.1** Git diff integration
  - [ ] Use `git diff HEAD~1 HEAD -- .github/scripts/listings.json`
  - [ ] Parse diff output to identify changed entries
  - [ ] Only process modified/added/removed entries
- [ ] **8.2** Change type detection
  - [ ] Identify additions, modifications, deletions
  - [ ] Handle role status changes (active -> inactive)
  - [ ] Track position changes within file
- [ ] **8.3** Incremental processing
  - [ ] Cache current state more efficiently
  - [ ] Only update Discord for actual changes
  - [ ] Reduce memory usage for large datasets
- [ ] **8.4** Performance monitoring
  - [ ] Add timing metrics for processing phases
  - [ ] Monitor memory usage improvements
  - [ ] Log statistics about change volumes
- [ ] **8.5** Fallback mechanism
  - [ ] Full processing mode for edge cases
  - [ ] Recovery from delta processing errors

**Files to modify**: `chatd/repo.py`, `chatd/storage.py`, `chatd/bot.py`

### 9. Role Status Management
**Goal**: Handle role deactivations and visibility changes

**Current Behavior**: Only posts new roles, ignores status changes
**Target**: Update/modify past messages when roles change status

**Implementation Plan**:
- [ ] **9.1** Message tracking enhancement
  - [ ] Store Discord message IDs with role keys
  - [ ] Track message-to-role mapping in storage
  - [ ] Add message update capabilities
- [ ] **9.2** Status change detection
  - [ ] Compare `visible` and `active` flags between updates
  - [ ] Identify roles that changed from active to inactive
  - [ ] Track roles that became hidden/invisible
- [ ] **9.3** Message modification strategies
  - [ ] **Option A**: Edit original message with strikethrough text
  - [ ] **Option B**: Add reaction (‚ùå) to indicate closure
  - [ ] **Option C**: Reply with update status
  - [ ] **Option D**: Delete message entirely
- [ ] **9.4** Configuration options
  - [ ] `HANDLE_DEACTIVATIONS=true`
  - [ ] `DEACTIVATION_STRATEGY=edit|react|reply|delete`
  - [ ] `DEACTIVATION_MESSAGE="üö´ This position is no longer available"`
- [ ] **9.5** Bulk status processing
  - [ ] Handle multiple simultaneous status changes
  - [ ] Rate limit message updates to avoid API limits
  - [ ] Error handling for messages that can't be modified

**Files to modify**: `chatd/storage.py`, `chatd/bot.py`, `chatd/messages.py`, `chatd/config.py`

### 10. Database Implementation (PostgreSQL + Docker) üóÑÔ∏è **(High Priority)**
**Goal**: Replace JSON file storage with PostgreSQL database for improved data management, querying, and scalability

**Current State**: Job postings stored in `previous_data.json` (~500KB), message tracking in `message_tracking.json` (~100KB)
**Target**: Normalized PostgreSQL database with cloud-agnostic Docker deployment

**Benefits**: 
- **Improved data integrity**: ACID compliance, foreign key constraints, data validation
- **Better querying**: SQL queries for analytics, filtering, and reporting
- **Normalized storage**: Separate tables for locations, terms, and message tracking
- **Universal deployment**: Works identically on AWS, Google Cloud, Azure, on-premises
- **Backup/restore**: Standard PostgreSQL tools for data migration
- **Scalability**: Handle growing dataset more efficiently than JSON files

**Storage Requirements**:
- **PostgreSQL Docker image**: ~80MB (postgres:15-alpine)
- **Initial database**: ~50MB + data growth (~8-12MB per 10k job postings)
- **Total baseline**: ~130-140MB + data growth (fits current disk constraints)

**Implementation Plan**:
- [x] **10.1: Database Infrastructure Setup**
  - [x] Created `docker-compose.database.yml` with PostgreSQL 15 Alpine service
  - [x] Designed normalized schema with 4 tables + 1 readable view  
  - [x] Implemented database initialization with `sql/init/001_initial_schema.sql`
  - [x] Added database validation script `scripts/test-database-setup.sh`
  - [x] Successfully deployed PostgreSQL container with health checks
  - [x] Verified schema creation and test data insertion
- [x] **10.2: Database Models & ORM**
  - [x] Created SQLAlchemy ORM models for type-safe database operations
  - [x] Implemented database factory pattern for connection management
  - [x] Added database configuration to `chatd/config.py`
  - [x] Created database abstraction layer in `chatd/database.py`
  - [x] Added PostgreSQL dependencies to `requirements.txt`
  - [x] Verified complete ORM functionality with test script
- [x] **10.3: Dual-write migration system**
  - [x] Implemented storage abstraction layer supporting both JSON and PostgreSQL
  - [x] Created `DataStorage` class with pluggable backends (JSON/PostgreSQL)
  - [x] Added `MIGRATION_MODE` configuration: `json_only|dual_write|database_only`
  - [x] Ensured backward compatibility during transition period
  - [x] Comprehensive error handling with fallback to JSON if database fails
- [x] **10.4: Historical data migration**
  - [x] Create migration script to convert existing JSON files to database
  - [x] Implement data validation and integrity checks during migration
  - [x] Create timestamped backups of existing JSON files before migration
  - [x] Progress tracking and logging for large dataset migrations
  - [x] Verification system to ensure migration completeness and accuracy
- [x] **10.5: Bot Integration**
  - [x] Refactor main bot to use DataStorage instead of FileStorage from chatd/storage.py
  - [x] Update all storage access points in chatd/bot.py to use unified DataStorage interface
  - [x] Replace get_storage() calls with DataStorage instantiation
  - [x] Update method calls to match DataStorage interface (get_job_postings, save_job_postings, etc.)
  - [x] Test dual_write mode functionality in production environment
  - [x] Update all bot tests to work with new DataStorage interface with proper mocking
- [ ] **10.6: Update support**
  - [ ] Check if values have changed on previous posts
  - [ ] Optimize by checking key fields only: active, is_visible, date_updated
    - [ ] active changed: update value only
    - [ ] is_visible changed: update value only
    - [ ] date_updated changed: indicates posting is being corrected, update entire JobPosting
  - [ ] Ensure that the update logic is idempotent and handles concurrent changes gracefully
  - [ ] Apply same changes to data migration script, including dry-run mode

**Phase 1 Results Achieved**:
- **PostgreSQL 15 Alpine**: Successfully deployed in Docker container with health checks
- **Normalized Database Schema**: 4 tables (job_postings, job_locations, job_terms, message_tracking) + readable view
- **Schema Validation**: Test data successfully inserted and retrievable
- **Database Infrastructure**: Ready for Python ORM integration
- **Storage Requirements Met**: ~130-140MB baseline within current disk constraints

**Phase 2 Results Achieved**:
- **SQLAlchemy ORM Models**: Complete type-safe models for all database tables with relationships
- **Database Connection Management**: Factory pattern with session scope and connection pooling
- **Configuration Integration**: Database settings fully integrated into config system
- **CRUD Operations**: Full Create, Read, Update, Delete functionality verified
- **Data Conversion**: Seamless conversion between ORM objects and dictionary format
- **Test Coverage**: Comprehensive test script validates all ORM functionality

**Phase 4 Results Achieved**:
- **Comprehensive migration script**: Full migration system with CLI interface and production support
- **Data validation**: Complete integrity checks with mismatch detection and reporting  
- **Backup system**: Automatic timestamped backups with rollback capabilities
- **Progress tracking**: Real-time progress with detailed logging (9,884 jobs processed)
- **Error handling**: Robust error management with transaction rollback and cleanup
- **Test coverage**: 27 comprehensive test cases covering all migration scenarios (100% pass rate)
- **Production ready**: Correct file paths and validation for deployment environment

**Phase 5 Results Achieved**:
- **Complete bot refactoring**: Main bot now uses DataStorage instead of legacy FileStorage
- **Method mapping**: Updated all storage calls to use new DataStorage interface methods
  - `save_message_info` ‚Üí `add_message_tracking`
  - `load_data` ‚Üí `get_job_postings`
  - `save_data` ‚Üí `save_job_postings`
  - `get_messages_for_role` ‚Üí `get_message_tracking` (with updated logic)
- **Comprehensive test updates**: All 15 bot tests updated and passing with proper DataStorage mocking
- **New functionality test**: Added test for `check_for_new_roles` function covering the main bot workflow
- **Ready for dual-write**: Bot now fully compatible with json_only, dual_write, and database_only modes

**Migration Script Features**:
```python
class DataMigrator:
    # Complete migration system with validation, backups, progress tracking
    def migrate_data(self, dry_run=False, verify=True):
        # Production path: /var/lib/chatd/data/previous_data.json
        # Features: validation, backup, verification, progress tracking
```

**Test Coverage**:
- 27 test cases in `tests/test_migration.py`
- Comprehensive error handling and edge case testing
- Mock-based testing with proper fixtures and parametrization
- Integration testing with temporary file systems

**Files Created**: `scripts/migrate_json_to_database.py`, `tests/test_migration.py`
- **Storage Abstraction Layer**: Unified interface supporting JSON, PostgreSQL, and dual-write modes
- **Backend Architecture**: Pluggable storage backends with health checking and status monitoring
- **Migration Mode Support**: Three distinct modes (json_only, dual_write, database_only) for gradual migration
- **Error Resilience**: Automatic fallback to JSON storage when database operations fail
- **Configuration Integration**: MIGRATION_MODE setting integrated into environment configuration
- **Test Coverage**: Comprehensive pytest test suite with proper mocking for all migration modes

**Files Created**: `docker-compose.database.yml`, `sql/init/001_initial_schema.sql`, `scripts/test_database_setup.sh`, `chatd/database.py`, `tests/test_database_models.py`, `chatd/storage_abstraction.py`, `tests/test_storage_abstraction.py`
**Files Modified**: `chatd/config.py`, `requirements.txt`, `.env`, `.env.example`,  `chatd/bot.py`, `tests/test_bot.py`

**Database Schema Design**:
```sql
-- Main job postings table
CREATE TABLE job_postings (
    id UUID PRIMARY KEY,
    date_updated BIGINT NOT NULL,
    url TEXT NOT NULL UNIQUE,
    company_name TEXT NOT NULL,
    title TEXT NOT NULL,
    sponsorship TEXT,
    active BOOLEAN DEFAULT true,
    source TEXT,
    date_posted BIGINT,
    company_url TEXT,
    is_visible BOOLEAN DEFAULT true
);

-- Normalized locations table (one-to-many)
CREATE TABLE job_locations (
    id UUID REFERENCES job_postings(id) ON DELETE CASCADE,
    location TEXT NOT NULL,
    PRIMARY KEY (id, location)
);

-- Normalized terms table (one-to-many)
CREATE TABLE job_terms (
    id UUID REFERENCES job_postings(id) ON DELETE CASCADE,
    term TEXT NOT NULL,
    PRIMARY KEY (id, term)
);

-- Message tracking table (one-to-one)
CREATE TABLE message_tracking (
    id UUID PRIMARY KEY REFERENCES job_postings(id) ON DELETE CASCADE,
    message_id TEXT NOT NULL,
    channel_id TEXT NOT NULL,
    posted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(message_id, channel_id)
);

-- Performance indexes
CREATE INDEX idx_job_postings_company ON job_postings(company_name);
CREATE INDEX idx_job_postings_active ON job_postings(active, is_visible);
CREATE INDEX idx_job_postings_date_posted ON job_postings(date_posted DESC);
CREATE INDEX idx_message_tracking_message_id ON message_tracking(message_id);
```

**Docker Deployment Configuration**:
```yaml
# docker-compose.database.yml
version: '3.8'
services:
  chatd-postgres:
    image: postgres:15-alpine
    container_name: chatd-postgres
    environment:
      POSTGRES_DB: chatd
      POSTGRES_USER: chatd
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/init:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"  # Only for development/debugging
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U chatd"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:
    driver: local
```

**Migration Process**:
```bash
# Phase 1: Deploy database infrastructure
sudo chatd db setup --init-schema

# Phase 2: Preview migration (dry run)  
sudo chatd db migrate --dry-run

# Phase 3: Execute migration with verification
sudo chatd db migrate --verify

# Phase 4: Switch to dual-write mode
echo "MIGRATION_MODE=dual_write" >> /etc/chatd/.env
sudo chatd restart

# Phase 5: Monitor and switch to database-only
echo "MIGRATION_MODE=database_only" >> /etc/chatd/.env
sudo chatd restart

# Phase 6: Verify and cleanup
sudo chatd db verify --integrity-check
sudo chatd db backup --compress
```

**New Management Commands**:
```bash
# Database management
sudo chatd db status                 # Show database connection and health
sudo chatd db setup                  # Initialize database schema
sudo chatd db migrate                # Migrate JSON data to database
sudo chatd db backup                 # Create compressed database backup
sudo chatd db restore backup.sql     # Restore from backup file
sudo chatd db verify                 # Verify data integrity
sudo chatd db cleanup               # Vacuum and optimize database
sudo chatd db query "SELECT COUNT(*) FROM job_postings"  # Execute SQL

# Migration management  
sudo chatd migrate status            # Show current migration mode
sudo chatd migrate preview           # Preview migration without changes
sudo chatd migrate execute           # Execute migration with verification
sudo chatd migrate rollback          # Emergency rollback to JSON-only mode
```

**Example Query Improvements**:
```python
# Before: Complex JSON iteration and filtering
def find_company_jobs(company_name, days=7):
    jobs = []
    for job in json_data:
        if (job['company_name'].lower() == company_name.lower() and 
            job['date_posted'] > (time.time() - days*24*3600)):
            jobs.append(job)
    return jobs

# After: Simple SQL query with database indexes
def find_company_jobs(company_name, days=7):
    cutoff_date = int(time.time() - days*24*3600)
    return session.query(JobPosting)\
        .filter(JobPosting.company_name.ilike(f'%{company_name}%'))\
        .filter(JobPosting.date_posted > cutoff_date)\
        .order_by(JobPosting.date_posted.desc())\
        .all()
```

**Configuration Options**:
```bash
# Database connection settings
DB_TYPE=postgresql                    # postgresql or sqlite (fallback)
DB_HOST=chatd-postgres               # Docker service name or hostname
DB_PORT=5432                         # Database port
DB_NAME=chatd                        # Database name
DB_USER=chatd                        # Database username
DB_PASSWORD=${POSTGRES_PASSWORD}     # Database password (from secrets)

# Migration settings
MIGRATION_MODE=json_only             # json_only|dual_write|database_only
DB_MIGRATION_BATCH_SIZE=100          # Records to migrate per batch
DB_BACKUP_RETENTION_DAYS=30          # Days to keep database backups
DB_CONNECTION_POOL_SIZE=5            # Connection pool size for performance

# Database maintenance
DB_AUTO_VACUUM=true                  # Enable automatic database maintenance
DB_BACKUP_SCHEDULE=daily             # Backup frequency (daily/weekly)
DB_HEALTH_CHECK_INTERVAL=300         # Health check interval in seconds
```

**Rollback Strategy**:
```bash
# Emergency rollback procedures
# 1. Switch back to JSON-only mode
echo "MIGRATION_MODE=json_only" >> /etc/chatd/.env
sudo chatd restart

# 2. Restore JSON files from backup if needed
sudo cp /var/lib/chatd/data/previous_data.json.backup.TIMESTAMP \
        /var/lib/chatd/data/previous_data.json
sudo cp /var/lib/chatd/data/message_tracking.json.backup.TIMESTAMP \
        /var/lib/chatd/data/message_tracking.json

# 3. Restart service
sudo chatd restart
```

**Files to create**:
- `chatd/database.py` (database abstraction layer and ORM models)
- `chatd/migrations.py` (data migration utilities)
- `sql/init/001_initial_schema.sql` (database schema initialization)
- `scripts/migrate-to-database.py` (comprehensive migration script)
- `docker-compose.database.yml` (PostgreSQL container configuration)

**Files to modify**:
- `chatd/storage.py` (add database backend support)
- `chatd/config.py` (database configuration options)
- `chatd/bot.py` (use database storage layer)
- `requirements.txt` (add psycopg2, SQLAlchemy dependencies)
- `scripts/create-management-scripts.sh` (database management commands)
- `Dockerfile` (database connectivity and initialization)

### 11. Enhanced Monitoring & Observability
**Goal**: Better visibility into bot performance and health

**Benefits**: Proactive issue detection, performance optimization insights, operational visibility

**Implementation Plan**:
- [ ] **11.1** Add metrics collection
  - [ ] Track messages processed per minute
  - [ ] Monitor Discord API rate limits and usage
  - [ ] Count successful vs failed operations
  - [ ] Memory and CPU usage tracking
- [ ] **11.2** Health check endpoint
  - [ ] Simple HTTP server for container health checks
  - [ ] Validate Discord connection status
  - [ ] Check git repository accessibility
  - [ ] Verify data directory write permissions
- [ ] **11.3** Alert system
  - [ ] Discord webhook for bot errors/failures
  - [ ] Email notifications for critical issues
  - [ ] Rate limit warnings
  - [ ] Repository sync failure alerts
- [ ] **11.4** Performance dashboards
  - [ ] Log parsing and visualization
  - [ ] Historical trend analysis
  - [ ] Repository processing time metrics
  - [ ] Error rate tracking

**Files to modify**: `chatd/bot.py`, `chatd/config.py`, `main.py`, `requirements.txt`

### 12. Configuration Validation & Safety ‚úÖ **COMPLETED**
**Goal**: Prevent misconfigurations and provide better error messages

**Benefits**: Faster debugging, prevents runtime failures, improves user experience

**Implementation Plan**:
- [x] **12.1** Startup validation
  - [x] Verify Discord token format and validity before starting
  - [x] Test channel access permissions and format validation
  - [x] Validate repository URL accessibility
  - [x] Check required environment variables
  - [x] Validate file system permissions for data directories
- [x] **12.2** Enhanced error reporting
  - [x] User-friendly error messages with emoji indicators
  - [x] Actionable troubleshooting advice in error messages
  - [x] Clear validation progress reporting
  - [x] Graceful startup failure with helpful guidance
- [x] **12.3** Comprehensive validation checks
  - [x] Discord token format validation (length and structure)
  - [x] Channel ID format and Discord snowflake validation
  - [x] Numeric configuration range validation (intervals, retries, etc.)
  - [x] Git command availability and repository access testing
  - [x] Directory write permission validation

**Results Achieved**:
- **Early error detection**: Configuration issues caught before bot startup
- **Better debugging**: Clear, actionable error messages with specific solutions
- **Professional validation**: Comprehensive checks for all critical components
- **User-friendly feedback**: Emoji-enhanced progress reporting and error descriptions
- **Robust startup**: Bot only starts if all validations pass

**Example validation output**:
```
üîç Starting configuration validation...
‚úÖ Discord token format validation passed
‚úÖ Channel IDs validation passed (2 channels configured)
‚úÖ Numeric configuration validation passed
‚úÖ File permissions validation passed
‚úÖ Repository accessibility validation passed
üîç Testing Discord connection...
‚úÖ Discord connection successful (logged in as ChatD Bot#1234)
‚úÖ Can access 2/2 configured channels
‚úÖ Configuration validation passed.
```

**Files modified**: `chatd/config.py`, `main.py`

### 13. Backup & Recovery System üéØ **(Stretch Goal)**
**Goal**: Automated backup and disaster recovery procedures

**Note**: Limited by device storage constraints - requires external storage solution

**Implementation Plan**:
- [ ] **13.1** Lightweight backup strategy
  - [ ] Configuration and critical data backup only
  - [ ] Compressed backup to external storage/cloud
  - [ ] Exclude logs and temporary files
- [ ] **13.2** Recovery procedures
  - [ ] One-command restore from backup
  - [ ] Database corruption recovery
  - [ ] Configuration restoration
- [ ] **13.3** External storage integration
  - [ ] Cloud storage backup (S3, Google Drive, etc.)
  - [ ] Remote server backup via SSH
  - [ ] USB/external drive backup support
- [ ] **13.4** Backup validation
  - [ ] Backup integrity checks
  - [ ] Minimal test restore procedures
  - [ ] Backup retention policies

**Files to modify**: `scripts/chatd-backup`, `chatd/config.py`, `scripts/recovery.sh`

### 14. Multi-Environment Support üéØ **(Requires Larger Disk)**
**Goal**: Support for development, staging, and production environments

**Benefits**: Safe testing, isolated development, professional deployment workflow, separate Discord bots for testing

**Prerequisites**: 
- **Larger microSD card** (current 7GB insufficient for multiple environments)
- **Separate Discord bot** for development environment
- **Environment isolation** to prevent dev changes affecting production

**Implementation Plan**:
- [ ] **14.1** Environment configuration structure
  - [ ] Separate Discord bots and tokens for dev/prod environments
  - [ ] Environment-specific Discord channels for testing
  - [ ] Environment-specific repository branches (dev/staging/main)
  - [ ] Isolated data storage per environment (`/var/lib/chatd-dev/`, `/var/lib/chatd-prod/`)
  - [ ] Environment-aware logging and monitoring
- [ ] **14.2** Configuration management
  - [ ] `.env.dev`, `.env.staging`, `.env.prod` configuration files
  - [ ] Environment inheritance (dev inherits from base, overrides specific values)
  - [ ] Separate Discord bot tokens and channel IDs per environment
  - [ ] Environment-specific repository URLs or branches
  - [ ] Configurable data directories and log files per environment
- [ ] **14.3** Development workflow improvements
  - [ ] Local development with test data and separate Discord bot
  - [ ] Development environment for testing changes safely
  - [ ] Environment promotion procedures (dev ‚Üí staging ‚Üí prod)
  - [ ] Configuration validation per environment
  - [ ] Isolated Docker containers per environment
- [ ] **14.4** Deployment strategies
  - [ ] Environment-specific systemd services (`chatd-dev`, `chatd-prod`)
  - [ ] Blue-green deployment support for production
  - [ ] Environment-specific Docker images and tags
  - [ ] Automated environment provisioning scripts
  - [ ] Environment health checks and monitoring
- [ ] **14.5** Environment management tools
  - [ ] `chatd env list` - Show available environments and their status
  - [ ] `chatd env switch <env>` - Switch active environment for operations
  - [ ] `chatd env status <env>` - Show specific environment status
  - [ ] `chatd env deploy <env>` - Deploy to specific environment
  - [ ] `chatd env logs <env>` - View logs for specific environment
  - [ ] Environment-specific configuration validation
- [ ] **14.6** Testing and promotion workflow
  - [ ] Deploy changes to dev environment first
  - [ ] Automated testing in dev environment
  - [ ] Manual approval process for production deployment
  - [ ] Rollback procedures for failed deployments
  - [ ] Configuration drift detection between environments
- [ ] **14.7** Environment-aware test simulation
  - [ ] Migrate existing `setup_test_update.sh` to support multiple environments
  - [ ] Environment-specific test data simulation (dev/staging/prod)
  - [ ] Safe testing of message replay functionality per environment
  - [ ] Automated test scenario generation for development
  - [ ] Test data isolation to prevent cross-environment contamination

**Example Environment Structure**:
```
/etc/chatd/
‚îú‚îÄ‚îÄ .env.dev          # Development configuration
‚îú‚îÄ‚îÄ .env.staging      # Staging configuration  
‚îú‚îÄ‚îÄ .env.prod         # Production configuration
‚îî‚îÄ‚îÄ .env.base         # Shared base configuration

/var/lib/chatd/
‚îú‚îÄ‚îÄ dev/              # Development data
‚îú‚îÄ‚îÄ staging/          # Staging data
‚îî‚îÄ‚îÄ prod/             # Production data

Docker containers:
‚îú‚îÄ‚îÄ chatd-dev         # Development bot
‚îú‚îÄ‚îÄ chatd-staging     # Staging bot
‚îî‚îÄ‚îÄ chatd-prod        # Production bot
```

**Example Configuration Structure**:
```bash
# .env.base (shared settings)
REPO_URL=https://github.com/builtbybob/chatd-internships.git
MAX_POST_AGE_DAYS=5
CHECK_INTERVAL_MINUTES=1

# .env.dev (development overrides)
DISCORD_TOKEN=<dev_bot_token>
CHANNEL_IDS=<dev_channel_ids>
LOCAL_REPO_PATH=/app/dev/Summer2026-Internships
DATA_FILE=/var/lib/chatd/dev/previous_data.json
LOG_LEVEL=DEBUG
CHECK_INTERVAL_MINUTES=5

# .env.prod (production overrides)  
DISCORD_TOKEN=<prod_bot_token>
CHANNEL_IDS=<prod_channel_ids>
LOCAL_REPO_PATH=/app/prod/Summer2026-Internships
DATA_FILE=/var/lib/chatd/prod/previous_data.json
LOG_LEVEL=INFO
```

**Example Management Commands**:
```bash
# Environment management
sudo chatd env list                    # Show all environments
sudo chatd env status dev              # Check dev environment
sudo chatd env switch dev              # Set dev as active environment

# Environment-specific operations
sudo chatd-dev status                  # Dev environment status
sudo chatd-dev logs -f                 # Follow dev logs
sudo chatd-dev build                   # Build dev image
sudo chatd-dev deploy                  # Deploy to dev

sudo chatd-prod status                 # Production status
sudo chatd-prod deploy                 # Deploy to production
sudo chatd-prod rollback               # Rollback production

# Testing workflow
sudo chatd env deploy dev              # Deploy changes to dev first
sudo chatd env test dev                # Run tests in dev environment
sudo chatd env promote dev staging     # Promote dev to staging
sudo chatd env promote staging prod    # Promote staging to prod
```

**Disk Space Requirements**:
- **Current**: ~7GB total (90% full)
- **Multi-environment**: ~15-20GB recommended
  - Base system: ~2GB
  - Production environment: ~5GB  
  - Development environment: ~3GB
  - Staging environment: ~3GB
  - Docker images (multiple): ~3GB
  - Logs and backups: ~2GB
  - Buffer space: ~2GB

**Migration Dependencies**:
- ‚úÖ **Migration scripts ready** (created in previous work)
- ‚è≥ **Larger microSD card** (waiting for physical access)
- ‚è≥ **Separate Discord bot creation** (requires Discord Developer Portal access)
- ‚è≥ **Additional disk space** (current 649MB free insufficient)

**Files to create**: 
- `.env.base`, `.env.dev`, `.env.staging`, `.env.prod`
- `scripts/setup-environments.sh`
- `scripts/environment-manager.sh` 
- `scripts/setup-test-update-multi-env.sh` (enhanced version of existing script)
- `chatd-internships-dev.service`, `chatd-internships-staging.service`

**Files to modify**: 
- `chatd/config.py` (environment detection and inheritance)
- `scripts/create-management-scripts.sh` (multi-environment commands)
- `chatd-internships.service` (production-specific service)
- `Dockerfile` (environment-aware builds)

### 15. Enhanced Test Simulation Framework üß™ **(Depends on Multi-Environment)**
**Goal**: Migrate and enhance existing test simulation script for multi-environment support

**Current State**: `setup_test_update.sh` allows replaying message updates by resetting to older commits
**Target**: Environment-aware testing with isolated test data per environment

**Implementation Plan**:
- [ ] **15.1** Migrate existing test script to multi-environment
  - [ ] Create `setup-test-update-multi-env.sh` to replace current script
  - [ ] Environment parameter support: `./setup-test-update-multi-env.sh dev 3`
  - [ ] Support both commit count and specific commit hash: `./setup-test-update-multi-env.sh dev abc123f`
  - [ ] Automatic detection of commit hash vs number format
  - [ ] Environment-specific data paths and repository locations
  - [ ] Isolated test data per environment (no cross-contamination)
  - [ ] Environment-aware restoration procedures
- [ ] **15.2** Enhanced testing capabilities
  - [ ] Predefined test scenarios (small update, large batch, edge cases)
  - [ ] Test scenario library with known expected outcomes
  - [ ] Automated verification of bot responses to test data
  - [ ] Performance benchmarking during test runs
  - [ ] Test result logging and comparison
- [ ] **15.3** Development workflow integration
  - [ ] Quick test commands: `chatd test replay dev 5` (replay 5 commits in dev)
  - [ ] Integration with environment deployment workflow
  - [ ] Automated testing as part of promotion pipeline
  - [ ] Test data reset and cleanup procedures
  - [ ] Safe testing isolation (never affect production data)
- [ ] **15.4** Test data management
  - [ ] Environment-specific previous_data.json files
  - [ ] Test scenario snapshots and restoration points
  - [ ] Automated test data generation from real data
  - [ ] Test data anonymization for development
  - [ ] Version control for test scenarios

**Example Enhanced Usage**:
```bash
# Environment-specific testing with commit counts
./scripts/setup-test-update-multi-env.sh dev 3        # Test 3 commits back in dev
./scripts/setup-test-update-multi-env.sh staging 5    # Test 5 commits back in staging

# Environment-specific testing with specific commit hashes
./scripts/setup-test-update-multi-env.sh dev abc123f  # Test specific commit in dev
./scripts/setup-test-update-multi-env.sh staging 9f8e7d6c # Test specific commit in staging

# Quick test commands (after multi-env setup)
sudo chatd test replay dev 3                          # Replay test in dev environment
sudo chatd test replay dev abc123f                    # Replay specific commit in dev
sudo chatd test scenario dev batch-update             # Run predefined test scenario
sudo chatd test reset dev                             # Reset dev environment test data
sudo chatd test verify dev                            # Verify test results

# Safe production testing (read-only)
sudo chatd test simulate prod 2                       # Simulate without affecting prod data
sudo chatd test simulate prod abc123f                 # Simulate specific commit in read-only mode
```

**Example Script Structure**:
```bash
#!/bin/bash
# setup-test-update-multi-env.sh
# Enhanced test simulation with environment support

ENVIRONMENT=${1:-dev}    # Default to dev environment
COMMIT_REF=${2:-2}       # Default to 2 commits back, or accept specific commit hash

# Detect if COMMIT_REF is a commit hash or number
if [[ "$COMMIT_REF" =~ ^[a-f0-9]{6,40}$ ]]; then
    # It's a commit hash
    RESET_TARGET="$COMMIT_REF"
    echo "Using specific commit: $COMMIT_REF"
elif [[ "$COMMIT_REF" =~ ^[0-9]+$ ]]; then
    # It's a number of commits back
    RESET_TARGET="HEAD~$COMMIT_REF"
    echo "Going back $COMMIT_REF commits"
else
    echo "ERROR: Invalid commit reference. Use either a number (e.g., 3) or commit hash (e.g., abc123f)"
    exit 1
fi

# Environment-specific paths
case "$ENVIRONMENT" in
    dev)
        REPO_DIR="/app/dev/Summer2026-Internships"
        DATA_FILE="/var/lib/chatd/dev/previous_data.json"
        ;;
    staging)
        REPO_DIR="/app/staging/Summer2026-Internships"
        DATA_FILE="/var/lib/chatd/staging/previous_data.json"
        ;;
    prod)
        echo "ERROR: Direct production testing not allowed. Use simulate mode."
        exit 1
        ;;
    *)
        echo "ERROR: Unknown environment: $ENVIRONMENT"
        exit 1
        ;;
esac

# Validate environment is safe for testing
if [[ "$ENVIRONMENT" == "prod" ]]; then
    echo "Production environment testing requires special approval"
    exit 1
fi

# Reset to specified commit or commit count
echo "Resetting $ENVIRONMENT repository to: $RESET_TARGET"
cd "$REPO_DIR" || exit 1
git reset --hard "$RESET_TARGET"

# Environment-aware test setup...
```

**Integration with Multi-Environment**:
- **Development**: Full test replay capabilities with message posting
- **Staging**: Controlled testing with limited Discord channels  
- **Production**: Read-only simulation mode (no actual messages sent)
- **Isolation**: Each environment has separate test data and repositories

**Benefits**:
- **Safe development testing** with realistic data scenarios
- **Automated test verification** of bot behavior changes
- **Performance benchmarking** during feature development
- **Regression testing** to ensure changes don't break existing functionality
- **Environment isolation** prevents test data from affecting production

**Files to create**:
- `scripts/setup-test-update-multi-env.sh` (enhanced multi-environment version)
- `scripts/test-scenarios/` (directory for predefined test cases)
- `scripts/test-verification.sh` (automated test result verification)

**Files to modify**:
- `setup_test_update.sh` (mark as deprecated, reference new script)

### 16. Monitoring Dashboard & Alerting System üìä **(High Priority)**
**Goal**: Comprehensive monitoring dashboard with real-time metrics, alerts, and historical analytics

**Benefits**: Proactive issue detection, performance insights, usage analytics, operational visibility

**Free Framework Options**:
- **Option A**: **Grafana + Prometheus + AlertManager** (Most popular, enterprise-grade)
- **Option B**: **InfluxDB + Telegraf + Grafana** (Time-series focused, great for metrics)
- **Option C**: **Elastic Stack (ELK)** (Elasticsearch + Logstash + Kibana - log-centric)
- **Option D**: **Zabbix** (All-in-one monitoring solution)

**Recommended Stack**: **Grafana + Prometheus + AlertManager** (industry standard, excellent Docker support)

**Implementation Plan**:
- [ ] **16.1** Metrics collection and export
  - [ ] Add Prometheus metrics endpoint to ChatD bot (`/metrics`)
  - [ ] Custom metrics for job postings, reactions, errors, response times
  - [ ] Discord API rate limit monitoring and usage tracking
  - [ ] System resource metrics (CPU, memory, disk, network)
  - [ ] Git repository sync metrics and timing
- [ ] **16.2** Prometheus setup and configuration
  - [ ] Prometheus server Docker container configuration
  - [ ] Metrics scraping configuration for ChatD bot
  - [ ] System metrics collection with node_exporter
  - [ ] Docker metrics collection with cadvisor
  - [ ] Retention policies and storage optimization
- [ ] **16.3** Grafana dashboard development
  - [ ] Real-time operations dashboard (job posts, errors, performance)
  - [ ] Historical analytics dashboard (trends, usage patterns)
  - [ ] System health dashboard (resources, uptime, alerts)
  - [ ] Business metrics dashboard (job posting statistics, engagement)
  - [ ] Alert status and incident management dashboard
- [ ] **16.4** AlertManager configuration
  - [ ] Error rate alerts (service failures, Discord API errors)
  - [ ] Performance alerts (slow response times, high memory usage)
  - [ ] Business logic alerts (no job posts for X hours, repo sync failures)
  - [ ] System alerts (disk space, high CPU, container restarts)
  - [ ] Alert routing and notification channels (Discord webhooks, email)
- [ ] **16.5** Integration and automation
  - [ ] Environment-specific monitoring (dev/staging/prod dashboards)
  - [ ] Automated dashboard provisioning and backup
  - [ ] Monitoring stack deployment automation
  - [ ] Integration with existing Docker and systemd infrastructure
  - [ ] Monitoring data backup and disaster recovery

**Key Metrics to Track**:
```yaml
# Business Metrics
- chatd_job_posts_total (counter)
- chatd_reactions_added_total (counter) 
- chatd_dm_messages_sent_total (counter)
- chatd_companies_processed_total (counter)
- chatd_duplicate_posts_filtered_total (counter)

# Performance Metrics  
- chatd_processing_duration_seconds (histogram)
- chatd_discord_api_requests_total (counter)
- chatd_discord_api_rate_limit_remaining (gauge)
- chatd_git_sync_duration_seconds (histogram)
- chatd_memory_usage_bytes (gauge)

# Error Metrics
- chatd_errors_total{type="discord_api|git_sync|processing"} (counter)
- chatd_failed_channels_total (counter)
- chatd_service_restarts_total (counter)

# System Metrics
- chatd_uptime_seconds (counter)
- chatd_last_successful_sync_timestamp (gauge)
- chatd_config_reload_total (counter)
```

**Dashboard Examples**:

**Real-time Operations Dashboard**:
- Live job posting rate (jobs/hour)
- Current processing status and queue length
- Discord API rate limit status
- Active errors and alerts
- Service uptime and health status

**Business Analytics Dashboard**:
- Daily/weekly job posting trends
- Top companies by job count
- Reaction engagement rates
- Popular job categories and locations
- Historical growth metrics

**System Health Dashboard**:
- CPU, memory, disk usage over time
- Container restart frequency
- Network I/O and Discord API performance
- Git repository sync health
- Log error rates and patterns

**Example Alert Rules**:
```yaml
# Service down alert
- alert: ChatDServiceDown
  expr: up{job="chatd"} == 0
  for: 1m
  annotations:
    summary: "ChatD service is down"
    description: "ChatD bot has been down for more than 1 minute"

# High error rate alert  
- alert: HighErrorRate
  expr: rate(chatd_errors_total[5m]) > 0.1
  for: 2m
  annotations:
    summary: "High error rate in ChatD"
    description: "Error rate is {{ $value }} errors/second"

# No job posts alert
- alert: NoJobPostsDetected
  expr: increase(chatd_job_posts_total[1h]) == 0
  for: 2h
  annotations:
    summary: "No job posts detected in 2 hours"
    description: "ChatD hasn't posted any jobs in the last 2 hours"

# High memory usage alert
- alert: HighMemoryUsage
  expr: chatd_memory_usage_bytes > 500 * 1024 * 1024  # 500MB
  for: 5m
  annotations:
    summary: "ChatD high memory usage"
    description: "Memory usage is {{ $value | humanize }}B"
```

**Docker Stack Configuration**:
```yaml
# docker-compose.monitoring.yml
version: '3.8'
services:
  prometheus:
    image: prom/prometheus:latest
    container_name: chatd-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  grafana:
    image: grafana/grafana:latest
    container_name: chatd-grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=chatd_monitoring_2025

  alertmanager:
    image: prom/alertmanager:latest
    container_name: chatd-alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml

volumes:
  prometheus_data:
  grafana_data:
```

**Integration with ChatD Bot**:
```python
# Add to chatd/bot.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# Metrics
job_posts_total = Counter('chatd_job_posts_total', 'Total job posts sent')
reactions_total = Counter('chatd_reactions_added_total', 'Total reactions added')
processing_time = Histogram('chatd_processing_duration_seconds', 'Processing time')
discord_api_requests = Counter('chatd_discord_api_requests_total', 'Discord API requests')
errors_total = Counter('chatd_errors_total', 'Total errors', ['type'])

# Start metrics server
start_http_server(8000)  # Metrics available at http://localhost:8000/metrics
```

**Notification Channels**:
- **Discord webhooks** for immediate alerts
- **Email notifications** for critical issues
- **SMS alerts** for service down situations (via email-to-SMS)
- **Slack integration** if using team communication

**Disk Space Requirements**:
- **Monitoring stack**: ~2-3GB additional space
- **Metrics retention**: ~500MB per month (configurable)
- **Log storage**: ~1GB per month (configurable)
- **Total additional**: ~3-5GB for comprehensive monitoring

**Files to create**:
- `docker-compose.monitoring.yml` (monitoring stack)
- `monitoring/prometheus.yml` (Prometheus configuration)
- `monitoring/alertmanager.yml` (alert configuration)
- `monitoring/grafana/dashboards/` (dashboard definitions)
- `scripts/setup-monitoring.sh` (automated setup)
- `scripts/monitoring-backup.sh` (dashboard and config backup)

**Files to modify**:
- `chatd/bot.py` (add Prometheus metrics)
- `chatd/config.py` (monitoring configuration options)
- `requirements.txt` (add prometheus_client)
- `Dockerfile` (expose metrics port)
- `README.md` (monitoring setup documentation)


### 17. Discord Message Update Integration
**Goal**: Update previously sent Discord messages based on database changes to job postings

**Dependencies**: Requires Phase 6 (Database Update Support) to be completed first

**Benefits**: Make the Discord chat reliably searchable, with up-to-date information that reflects current job status

**Behavior**: Discord messages will be managed as follows:
- **Hidden posts** (`is_visible = false`): Delete Discord message entirely
- **Inactive posts** (`active = false`): Apply strikethrough formatting to indicate closure
- **Updated information** (`date_updated` change): Edit message content and add "Updated on [date]" footer

**Implementation Plan**:
- [ ] **17.1: Change Detection Integration**
  - [ ] Integrate with Phase 6 change detection to identify Discord message updates needed
  - [ ] Subscribe to database change events from DataStorage layer
  - [ ] Queue Discord message updates asynchronously to avoid blocking database operations
  - [ ] Handle bulk change scenarios (multiple jobs updated simultaneously)
- [ ] **17.2: Discord Message Management**
  - [ ] Implement message deletion for hidden posts (`is_visible = false`)
  - [ ] Implement strikethrough formatting for inactive posts (`active = false`)
  - [ ] Implement message content updates for changed job information (`date_updated`)
  - [ ] Add "Updated on [date]" footer for modified job postings
  - [ ] Handle Discord API rate limits and retry logic for message operations
- [ ] **17.3: Message Operation Safety**
  - [ ] Verify message still exists before attempting updates (handle deleted messages gracefully)
  - [ ] Implement idempotent message operations (avoid duplicate updates)
  - [ ] Handle permissions errors (bot may lose message edit permissions)
  - [ ] Add fallback strategies when message updates fail
  - [ ] Track message update status in database for auditing
- [ ] **17.4: Integration with Bot Logic**
  - [ ] Modify main bot sync loop to process both new jobs and message updates
  - [ ] Separate processing queues for new messages vs. updates to existing messages
  - [ ] Ensure message updates don't interfere with new job posting workflow
  - [ ] Add configuration options for enabling/disabling message updates
  - [ ] Implement dry-run mode for testing message update logic
- [ ] **17.5: Error Handling and Resilience**
  - [ ] Graceful degradation when Discord API is unavailable
  - [ ] Retry logic with exponential backoff for failed message operations
  - [ ] Dead letter queue for message updates that repeatedly fail
  - [ ] Logging and monitoring for message update success/failure rates
  - [ ] Alert system for high message update failure rates
- [ ] **17.6: Testing and Validation**
  - [ ] Comprehensive test suite for all message update scenarios
  - [ ] Integration tests with mock Discord API
  - [ ] Test rate limit handling and retry logic
  - [ ] Validate message formatting (strikethrough, update footers)
  - [ ] Performance testing for bulk message update scenarios

**Configuration Options**:
```bash
# Discord message update settings
ENABLE_MESSAGE_UPDATES=true               # Enable/disable message update feature
MESSAGE_UPDATE_STRATEGY=edit              # edit|delete|strikethrough for inactive jobs
MESSAGE_UPDATE_BATCH_SIZE=10              # Process updates in batches
MESSAGE_UPDATE_RETRY_COUNT=3              # Retry failed updates
MESSAGE_UPDATE_RATE_LIMIT_DELAY=1000      # Delay between updates (ms)
MESSAGE_UPDATE_DRY_RUN=false              # Test mode without actual Discord operations

# Update formatting options
STRIKETHROUGH_INACTIVE_JOBS=true          # Apply strikethrough to inactive jobs
DELETE_HIDDEN_JOBS=true                   # Delete messages for hidden jobs  
ADD_UPDATE_FOOTER=true                    # Add "Updated on" footer for changes
UPDATE_FOOTER_FORMAT="Updated on {date}"  # Customizable update footer text
```

**Example Message Update Behaviors**:
```markdown
# Original job posting
üÜï **Software Engineering Intern** at **TechCorp**
üìç San Francisco, CA | Remote
üíº Summer 2025 Internship
üîó Apply: https://techcorp.com/jobs/123

# After job becomes inactive (active = false)
~~üÜï **Software Engineering Intern** at **TechCorp**~~
~~üìç San Francisco, CA | Remote~~
~~üíº Summer 2025 Internship~~
~~üîó Apply: https://techcorp.com/jobs/123~~

# After job information is updated (date_updated changed)
üÜï **Senior Software Engineering Intern** at **TechCorp** 
üìç San Francisco, CA | Remote | New York, NY
üíº Summer 2025 Internship | Fall 2025 Co-op
üîó Apply: https://techcorp.com/jobs/123

*Updated on September 27, 2025*

# After job becomes hidden (is_visible = false)
[Message deleted from Discord]
```

**Files to modify**: `chatd/bot.py`, `chatd/messages.py`, `chatd/storage_abstraction.py`, `chatd/config.py`
**Files to create**: `tests/test_message_updates.py`

## üìã Implementation Notes

### Development Workflow
1. **Create feature branches** for each TODO item
2. **Add unit tests** before implementing features
3. **Update configuration documentation** for any new settings
4. **Test in development environment** before production deployment
5. **Update README.md** with new features and configuration options

### Configuration Management
- All new settings should have sensible defaults
- Add validation for configuration values
- Document impact and recommended values
- Consider backward compatibility

### Testing Strategy
- **Unit tests**: Individual component testing
- **Integration tests**: Full workflow testing
- **Performance tests**: Benchmark improvements
- **Production validation**: Test with real data in controlled manner

### Deployment Considerations
- **Zero-downtime deployment** for non-breaking changes
- **Database migration strategy** for storage changes
- **Rollback procedures** for failed deployments
- **Monitoring and alerting** for new features

---

## ‚úÖ Completed Improvements

### **Dynamic Log Level Control** *(September 22, 2025)*
**Problem**: Log level changes required service restart, causing production downtime
- No way to debug production issues without interrupting service
- Debugging required stopping bot, changing config, and restarting
- Lost ability to investigate transient issues in real-time

**Solution**: Runtime log level control without restart
- **Added** `chatd-loglevel` command supporting all 5 log levels
- **Implemented** SIGHUP signal handler for instant level changes
- **Created** Docker-compatible file-based communication system
- **Simplified** implementation with clean, direct level setting

**Impact**:
- [x] **Instant debugging**: Change to DEBUG level without any service interruption
- [x] **Production troubleshooting**: Investigate live issues immediately
- [x] **Professional interface**: `sudo chatd-loglevel debug` provides immediate verbose logging
- [x] **Full control**: Support for debug/info/warning/error/critical levels
- [x] **Zero downtime**: Maintain service availability during troubleshooting

**Commands Available**:
```bash
sudo chatd-loglevel debug     # Maximum verbosity for troubleshooting
sudo chatd-loglevel info      # Normal operational messages  
sudo chatd-loglevel warning   # Warnings and errors only
sudo chatd-loglevel error     # Error conditions only
sudo chatd-loglevel critical  # Critical failures only
```

**Files Modified**: `chatd/logging_utils.py`, `scripts/create-management-scripts.sh`

### **Configurable Date Filtering** *(September 21, 2025)*
**Problem**: Hardcoded 5-day role filtering preventing flexible deployment configurations
- No way to adjust filtering without code changes
- Different environments might need different filtering windows
- Operational inflexibility for various use cases

**Solution**: Environment variable configuration with validation
- **Added** `MAX_POST_AGE_DAYS` environment variable (default: 5)
- **Implemented** 1-30 day range validation with helpful error messages
- **Enhanced** logging to show configured max age in debug output
- **Maintained** backward compatibility with existing behavior

**Impact**: 
- [x] Flexible role filtering based on deployment needs
- [x] Prevents spam from old bulk updates (configurable threshold)
- [x] No code changes required for operational adjustments
- [x] Clear validation prevents misconfiguration

**Files Modified**: `chatd/config.py`, `chatd/bot.py`, `.env.example`

### **Docker Build Performance Optimization** *(September 21, 2025)*
**Problem**: Slow deployments due to Docker rebuilding on every service restart
- `systemctl start` triggered full Docker rebuild (~4+ minutes)
- No separation between building images and deploying containers
- Made development iterations painfully slow

**Solution**: Separated build and deployment phases
- **Removed** Docker build from systemd service startup process
- **Added** dedicated management commands: `chatd build`, `chatd deploy`, `chatd update`
- **Optimized** deployment workflow for development speed

**Impact**: 
- [x] Deployment time reduced from ~4+ minutes to ~8 seconds
- [x] Build once, deploy multiple times capability
- [x] Faster iteration cycles for development and testing
- [x] Clear separation of concerns (build vs deploy)

**Files Modified**: `chatd-internships.service`, `scripts/create-management-scripts.sh`

### **ID-Based Role Tracking Implementation** *(September 24, 2025)*
**Problem**: Complex composite key matching system caused reliability issues
- Over-matching prevented detection of role re-openings
- Complex `get_role_id()` function with 28 lines of normalization logic
- Potential for matching ambiguity and missed notifications

**Solution**: Direct UUID-based tracking from listings.json
- **Migrated** from composite keys to direct `role['id']` access
- **Eliminated** complex `get_role_id()` and `normalize_role_key()` functions entirely
- **Implemented** comprehensive migration script for production data
- **Enhanced** management scripts with user-agnostic build process

**Impact**:
- [x] **100% reliable tracking**: Direct UUID access eliminates matching ambiguity
- [x] **Simplified codebase**: Removed 50+ lines of complex matching code  
- [x] **Production deployment**: Successfully migrated 319/325 messages (98.2% success)
- [x] **Future-proof architecture**: Scales with listings.json growth without complexity
- [x] **Enhanced tooling**: User-agnostic build system with CHATD_BRANCH support

**Migration Results**:
```
‚úÖ Production data migration completed
   ‚Ä¢ Messages migrated: 319/325 (98.2% success rate)
   ‚Ä¢ Backup created: message_tracking.json.backup.1758770169
   ‚Ä¢ 6 unmigrated messages: likely obsolete roles no longer in listings.json
```

**Files Modified**: `chatd/repo.py`, `chatd/bot.py`, `scripts/migrate-to-id-keys.py`, `scripts/create-management-scripts.sh`

### **Database Implementation (PostgreSQL + Docker)** *(September 26, 2025)*
**Problem**: JSON file storage limitations for growing dataset and complex queries
- Limited querying capabilities with flat JSON file storage
- No data integrity constraints or validation at storage level
- Scalability concerns with growing internship dataset
- Complex backup and recovery procedures with file-based storage

**Solution**: Complete PostgreSQL database implementation with migration system
- **Implemented** full PostgreSQL + Docker stack with normalized schema
- **Created** SQLAlchemy ORM models with relationships and type safety
- **Built** comprehensive migration system with dual-write capabilities
- **Designed** storage abstraction layer supporting JSON, PostgreSQL, and dual-write modes

**Implementation Phases**:
1. **Phase 1**: Database infrastructure setup with Docker and schema design ‚úÖ
2. **Phase 2**: SQLAlchemy ORM models with complete CRUD operations ‚úÖ
3. **Phase 3**: Storage abstraction layer with migration mode support ‚úÖ
4. **Phase 4**: Historical data migration script with validation and backup ‚úÖ

**Impact**:
- [x] **Improved data integrity**: ACID compliance with foreign key constraints
- [x] **Better querying**: SQL queries for analytics, filtering, and reporting
- [x] **Normalized storage**: Separate tables for locations, terms, and message tracking
- [x] **Universal deployment**: Docker-based solution works on any cloud platform
- [x] **Migration safety**: Comprehensive backup and rollback capabilities
- [x] **Test coverage**: 27 test cases with 100% pass rate for all migration scenarios

**Migration Results**:
```
‚úÖ Phase 4 (Historical Data Migration) completed successfully!
   ‚Ä¢ Jobs migrated: 9,884/9,884 (100% success rate)
   ‚Ä¢ Messages migrated: 0/0 (no existing messages to migrate)  
   ‚Ä¢ Migration time: 0:00:01.670746
   ‚Ä¢ Validation: All data integrity checks passed
```

**Database Schema**:
- **job_postings**: Main table with normalized job data and UUID keys
- **job_locations**: One-to-many locations with proper referential integrity
- **job_terms**: One-to-many employment terms (internship, co-op, etc.)
- **message_tracking**: One-to-one message tracking with cascade deletion
- **jobs_with_details**: Readable view combining all related data

**Files Created**: `docker-compose.database.yml`, `sql/init/001_initial_schema.sql`, `chatd/database.py`, `chatd/storage_abstraction.py`, `scripts/migrate_json_to_database.py`, `tests/test_migration.py`, `tests/test_database_models.py`, `tests/test_storage_abstraction.py`
**Files Modified**: `chatd/config.py`, `requirements.txt`, `.env.example`

---

## üìä Progress Tracking

- [x] **Critical Architectural Improvements**: 1/1 ‚úÖ (ID-based role tracking)
- [x] **Performance Improvements**: 1/1 ‚úÖ (Docker build optimization) 
- [x] **Configuration Enhancements**: 2/2 ‚úÖ (Configurable date filtering, configuration validation)
- [x] **Operational Improvements**: 1/1 ‚úÖ (Dynamic log level control)
- [x] **Infrastructure Projects**: 1/4 ‚úÖ (Database implementation complete; Docker auto-pruning, multi-environment support, enhanced test simulation, monitoring dashboard pending)
- [ ] **Feature Enhancements**: 0/4 (Async reactions, smart reactions, role status management, enhanced monitoring)
- [x] **Items Completed**: 6/15 ‚úÖ (Docker optimization, date filtering, log level control, config validation, ID-based tracking, database implementation)
- [ ] **Total Sub-tasks**: 40/75+ completed

**Immediate Quick Wins** üöÄ:
- Docker Image Auto-Pruning (can be implemented now, will free up disk space immediately)

**Blocked by Disk Migration**:
- Multi-Environment Support (requires ~15-20GB disk space, currently have 7GB total)
- Enhanced Test Simulation Framework (depends on multi-environment setup)
- Monitoring Dashboard & Alerting System (requires ~3-5GB additional space for monitoring stack)

*Last Updated: September 27, 2025*
